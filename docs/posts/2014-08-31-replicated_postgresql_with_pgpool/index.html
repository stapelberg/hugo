<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Replicated PostgreSQL with pgpool2</title>
    <meta name="viewport" content="width=device-width">
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta name="description" content="Michael Stapelberg’s private website, containing articles about computers and programming, mostly focused on Linux.">
    <link rel="canonical" href="https://michael.stapelberg.de/posts/2014-08-31-replicated_postgresql_with_pgpool/">
    <meta name="author" content="Michael Stapelberg">
    <meta name="keywords" content="Michael, Stapelberg, Linux, Debian">
    <style type="text/css">
@font-face {
  font-family: 'Yanone Kaffeesatz Regular';
  font-style: normal;
  font-weight: 400;
  src: local('Yanone Kaffeesatz Regular'), url('/YanoneKaffeesatz-Regular.ttf') format('truetype');
}

@font-face {
  font-family: 'Droid Sans';
  font-style: normal;
  font-weight: normal;
  src: local('Droid Sans'), local('DroidSans'), url('/droid_sans.ttf') format('truetype');
}

@font-face {
  font-family: 'Droid Sans';
  font-style: normal;
  font-weight: bold;
  src: local('Droid Sans Bold'), local('DroidSans-Bold'), url('/droid_sans_bold.ttf') format('truetype');
}

body, td, th {
  font-family: 'Droid Sans', Verdana, Arial, Helvetica, sans-serif;
  font-size: 16px;
  color: #000;
} 
    </style>
    <link rel="stylesheet" href="/2.css" type="text/css">
    <link rel="stylesheet" href="/1.css" type="text/css">
    <link rel="alternate" href="/feed.xml" type="application/atom+xml" title="ATOM Feed">
</head>


<body><div class="container-fluid"><div class="row"><div id="menu" class="col-md-2"><h1>Michael Stapelberg</h1><ul class="menu_open">
<li><a class="menu_item" href="/"><span class="menu_title">Startseite</span></a></li>
<li><a class="menu_item" href="/Artikel/"><span class="menu_title">Artikel</span></a></li>
<li><a class="menu_item" href="/Dokumente/"><span class="menu_title">Dokumente</span></a></li>
<li><a class="menu_item" href="/Programme/"><span class="menu_title">Programme</span></a>
<div class="subitem">
<ul class="menu_open">
  <li><a class="menu_item" href="https://gokrazy.github.io/"><span class="menu_title">gokrazy</span></a></li>
  <li><a class="menu_item" href="https://i3wm.org/"><span class="menu_title">i3</span></a></li>
<li><a class="menu_item" href="https://robustirc.net/"><span class="menu_title">RobustIRC</span></a></li>
<li><a class="menu_item" href="https://www.x11vis.org/"><span class="menu_title">x11vis</span></a></li>
<li><a class="menu_item" href="/c128_kasse"><span class="menu_title">c128_kasse</span></a></li>
</ul>
</div>
</li>

<li><a class="menu_item" href="https://www.debian.org/"><span class="menu_title">Debian</span></a>
<div class="subitem">
<ul class="menu_open">
  <li><a class="menu_item" href="https://codesearch.debian.net/"><span class="menu_title">Debian Code Search</span></a></li>
  <li><a class="menu_item" href="https://manpages.debian.org/"><span class="menu_title">manpages.debian.org</span></a></li>
</ul>
</div>
</li>

<li><a class="menu_item" href="/talks"><span class="menu_title">Talks</span></a></li>

<li><a class="menu_item" href="/Tipps"><span class="menu_title">Tipps</span></a></li>

<li><a class="menu_item" href="/MeinePCs"><span class="menu_title">Hardware</span></a></li>
<li><a class="menu_item" href="/Impressum"><span class="menu_title">Impressum</span></a></li></ul>

</div><div id="contentBox" class="col-md-10"><h2 class="title">Replicated PostgreSQL with pgpool2</h2> (<span id="date">2014-08-31</span>) <div class="Artikel" id="content">
<p>
I run multiple web services, mostly related to <a
href="http://i3wm.org/">i3wm.org</a>. All of them use PostgreSQL as their
database, so the data that is stored in that PostgreSQL database is pretty
important to me and the users of these services.
</p>

<p>
Since a while now, I have been thinking about storing that data in a more
reliable way. Currently, it is stored on a single server, and is backed up to
two different locations (one on-site, one off-site) every day. The server in
question has a RAID-1 of course, but still: the setup implies that if that one
server dies, the last backup may be about a day old in the worst case, and also
it could take me significant time to get the services back up.
</p>

<p>
The areas in which I’d like to improve my setup are thus:
</p>

<ol>
<li>
<strong>Durability:</strong> In case the entire server dies, I want to have an
up-to-date copy of all data.
</li>
<li>
<strong>Fault tolerance:</strong> In case the entire server dies, I want to be
able to quickly switch to a different server. A secondary machine should be
ready to take over, albeit not fully automatically because fully automatic
solutions typically are either fragile or require a higher number of servers
than I’m willing to afford.
</li>
</ol>

<p>
For PostgreSQL, there are various settings and additional programs that you can
use which will provide you with some sort of clustering/replication. There is
an overview <a
href="https://wiki.postgresql.org/wiki/Replication,_Clustering,_and_Connection_Pooling">in
the PostgreSQL wiki (“Replication, Clustering, and Connection Pooling”)</a>. My
solution of choice is <a
href="https://wiki.postgresql.org/wiki/Pgpool-II">pgpool2</a> because it seems
robust and mature (yet under active development) to me, it is reasonably well
documented and I think I roughly understand what it does under the covers.
</p>

<h3>The plan</h3>

<p>
I have two servers, located in different data centers, that I will use for this
setup. The number of servers does not really matter, meaning you can easily add
a third or fourth server (increasing latency with every server of course).
However, the low number of servers places some restrictions on what we can do.
As an example, solutions that involve global consistency based on paxos/raft
quorums will not work with only two servers. As a consequence, master election
is out of the question and a human will need to do the actual
failover/recovery.
</p>

<p>
Each of the two servers will run PostgreSQL, but only one of them will run
pgpool2 at a time. The DNS records for e.g. faq.i3wm.org will point to the
server on which pgpool2 is running, so that server handles 100% of the traffic.
Let’s call the server running pgpool2 the primary, and the other server the
secondary. All queries that modify the database will still be sent to the
secondary, but the secondary does not handle any user traffic. This could be
accomplished by either not running the applications in question, or by having
them connect to the pgpool2 on the primary.
</p>

<p>
When a catastrophe happens, the DNS records will be switched to point to the
old-secondary server, and pgpool2 will be started there. Once the old-primary
server is available again, it will become the secondary server, so that in case
of another catastrophe, the same procedure can be executed again.
</p>

<p>
With a solution that involves only two servers, an often encountered problem
are split-brain situations. This means both servers think they are primary,
typically because there is a network partition, meaning the servers cannot talk
to each other. In our case, it is important that user traffic is not handled by
the secondary server. This could happen after failing over because DNS heavily
relies on caching, so switching the record does not mean that suddenly all
queries will go to the other server — this will only happen over time. A
solution for that is to either kill pgpool2 manually if possible, or have a
tool that kills pgpool2 when it cannot verify that the DNS record points to the
server.
</p>

<h3>Configuration</h3>

<p>
I apologize for the overly long lines in some places, but there does not seem
to be a way to use line continuations in the PostgreSQL configuration file.
</p>

<h3>Installing and configuring PostgreSQL</h3>

<p>
The following steps need to be done on each database server, whereas pgpool2
will only be installed on precisely one server.
</p>

<p>
Also note that a prerequisite for the configuration described below is that
hostnames are configured properly on every involved server, i.e. <code>hostname
-f</code> should return the fully qualified hostname of the server in question,
and other servers must be able to connect to that hostname.
</p>

<pre>
apt-get install postgresql postgresql-9.4-pgpool2 rsync ssh
cat &gt;&gt;/etc/postgresql/9.4/main/postgresql.conf &lt;&lt;'EOT'
listen_addresses = '*'

max_wal_senders = 1
wal_level = hot_standby
archive_mode = on
archive_command = 'test ! -f /var/lib/postgresql/9.4/main/archive_log/backup_in_progress || (test -f /var/lib/postgresql/9.4/main/archive_log/%f || cp %p /var/lib/postgresql/9.4/main/archive_log/%f)'
EOT
install -o postgres -g postgres -m 700 -d \
  /var/lib/postgresql/9.4/main/archive_log
systemctl restart postgresql.service
</pre>

<p>
pgpool comes with an extension (implemented in C) that provides a couple of
functions which are necessary for recovery. We need to “create” the extension
in order to be able to use these functions. After running the following
command, you can double-check with <code>\dx</code> that the extension was
installed properly.
</p>
<pre>
echo 'CREATE EXTENSION "pgpool_recovery"' | \
  su - postgres -c 'psql template1'
</pre>

<p>
During recovery, pgpool needs to synchronize data between the PostgreSQL
servers. This is done partly by running <code>pg_basebackup</code> on the
recovery target via SSH and using <code>rsync</code> (which connects using
SSH). Therefore, we need to create a passwordless SSH key for the
<code>postgres</code> user. For simplicity, I am implying that you’ll copy the
same <code>id_rsa</code> and <code>authorized_keys</code> files onto every
database node. You’ll also need to connect to every other database server once
in order to get the SSH host fingerprints into the known_hosts file.
</p>
<pre>
su - postgres
ssh-keygen -f /var/lib/postgresql/.ssh/id_rsa -N ''
cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys
exit
</pre>

<p>
We’ll also need to access remote databases with <code>pg_basebackup</code>
non-interactively, so we need a password file:
</p>
<pre>
su - postgres
echo '*:*:*:postgres:wQgvBEusf1NWDRKVXS15Fc8' > .pgpass
chmod 0600 .pgpass
exit
</pre>

<p>
When pgpool recovers a node, it first makes sure the data directory is up to
date, then it starts PostgreSQL and tries to connect repeatedly. Once the
connection succeeded, the node is considered healthy. Therefore, we need to
give the <code>postgres</code> user permission to control
<code>postgresql.service</code>:
</p>
<pre>
apt-get install sudo
cat &gt;/etc/sudoers.d/pgpool-postgres &lt;&lt;'EOT'
postgres ALL=(ALL:ALL) NOPASSWD:/bin/systemctl start postgresql.service
postgres ALL=(ALL:ALL) NOPASSWD:/bin/systemctl stop postgresql.service
EOT
</pre>

<p>
Now enable password-based authentication for all databases and replication
traffic. In case your database nodes/clients don’t share a common hostname
suffix, you may need to use multiple entries or replace the hostname suffix by
“all”.
</p>
<pre>
cat >>/etc/postgresql/9.4/main/pg_hba.conf <<'EOT'
host    all             all             .zekjur.net             md5     
host    replication     postgres        .zekjur.net             md5     
EOT
</pre>

<p>
After enabling password-based authentication, we need to set a password for the
<code>postgres</code> user which we’ll use for making the base backup:
</p>

<pre>
echo "ALTER USER postgres WITH PASSWORD 'wQgvBEusf1NWDRKVXS15Fc8';" | \
  su postgres -c psql
</pre>

<h3>Installing pgpool2</h3>

<pre>
apt-get install pgpool2
cd /etc/pgpool2
gunzip -c /usr/share/doc/pgpool2/examples/\
pgpool.conf.sample-replication.gz > pgpool.conf
</pre>

<p>
To interact with pgpool2, there are a few command-line utilities whose name
starts with <code>pcp_</code>. In order for these to work, we must configure a
username and password. For simplicity, I’ll re-use the password we set earlier
for the <code>postgres</code> user, but you could chose to use an entirely
different username/password:
</p>
<pre>
echo "postgres:$(pg_md5 wQgvBEusf1NWDRKVXS15Fc8)" >> pcp.conf
</pre>

<p>
In replication mode, when the client should authenticate towards the PostgreSQL
database, we also need to tell pgpool2 that we are using password-based
authentication:
</p>
<pre>
sed -i 's/trust$/md5/g' pool_hba.conf
sed -i 's/\(enable_pool_hba =\) off/\1 on/g' pgpool.conf
</pre>

<p>
Furthermore, we need to provide all the usernames and passwords that we are going to use to pgpool2:
</p>
<pre>
touch pool_passwd
chown postgres.postgres pool_passwd
pg_md5 -m -u faq_i3wm_org secretpassword
</pre>

<p>
For the use-case I am describing here, it is advisable to turn off
<code>load_balance_mode</code>, otherwise queries will be sent to all healthy
backends, which is slow because they are not in the same network. In addition,
we’ll assign a higher weight to the backend which runs on the same machine as
pgpool2, so read-only queries are sent to the local backend only.
</p>
<pre>
sed -i 's/^load_balance_mode = on/load_balance_mode = off/g' \
    pgpool.conf
</pre>

<p>
Now, we need to configure the backends.
</p>
<pre>
sed -i 's/^\(backend_\)/# \1/g' pgpool.conf

cat >>pgpool.conf <<'EOT'
backend_hostname0 = 'midna.zekjur.net'
backend_port0 = 5432
backend_weight0 = 2
backend_data_directory0 = '/var/lib/postgresql/9.4/main'

backend_hostname1 = 'alp.zekjur.net'
backend_port1 = 5432
backend_weight1 = 1
backend_data_directory1 = '/var/lib/postgresql/9.4/main'
EOT
</pre>

<h3>Overview: How recovery works</h3>

<p>
Let’s assume that pgpool is running on <code>midna.zekjur.net</code> (so
<code>midna</code> is handling all the traffic), and
<code>alp.zekjur.net</code> crashed. pgpool will automatically degrade
<code>alp</code> and continue operation. When you tell it to recover
<code>alp</code> because the machine is available again, it will do three
things:
</p>

<ol>
<li>
(“1st stage”) SSH into <code>alp</code> and run pg_basebackup to get a copy of
<code>midna</code>’s database.
</li>
<li>
(“2nd stage”) Disconnect all clients so that the database on <code>midna</code>
will not be modified anymore. Flush all data to disk on <code>midna</code>,
then rsync the data to <code>alp</code>. pg_basebackup from 1st stage will have
copied almost all of it, so this is a small amount of data — typically on the
order of 16 MB, because that’s how big one WAL file is.
</li>
<li>
Try to start PostgreSQL on <code>alp</code> again. pgpool will wait for 90
seconds by default, and within that time PostgreSQL must start up in such a
state that pgpool can connect to it.
</li>
</ol>

<p>
So, during the 1st stage, which copies the entire database, traffic will still
be handled normally, only during 2nd stage and until PostgreSQL started up no
queries are served.
</p>

<h3>Configuring recovery</h3>

<p>
For recovery, we need to provide pgpool2 with a couple of shell scripts that
handle the details of how the recovery is performed.
</p>

<pre>
sed -i 's/^\(recovery_\|client_idle_limit_in_recovery\)/# \1/g' \
    pgpool.conf

cat >>pgpool.conf <<'EOT'
recovery_user = 'postgres'
recovery_password = 'wQgvBEusf1NWDRKVXS15Fc8'

# This script is being run by invoking the pgpool_recovery() function on
# the current master(primary) postgresql server. pgpool_recovery() is
# essentially a wrapper around system(), so it runs under your database
# UNIX user (typically "postgres").
# Both scripts are located in /var/lib/postgresql/9.4/main/
recovery_1st_stage_command = '1st_stage.sh'
recovery_2nd_stage_command = '2nd_stage.sh'

# Immediately disconnect all clients when entering the 2nd stage recovery
# instead of waiting for the clients to disconnect.
client_idle_limit_in_recovery = -1
EOT
</pre>

<p>
The 1st_stage.sh script logs into the backend that should be recovered and uses
<code>pg_basebackup</code> to copy a full backup from the master(primary)
backend. It also sets up the <code>recovery.conf</code> which will be used by
PostgreSQL when starting up.
</p>

<pre>
cat >/var/lib/postgresql/9.4/main/1st_stage.sh <<'EOF'
#!/bin/sh
TS=$(date +%Y-%m-%d_%H-%M-%S)
MASTER_HOST=$(hostname -f)
MASTER_DATA=$1
RECOVERY_TARGET=$2
RECOVERY_DATA=$3

# Move the PostgreSQL data directory out of our way.
ssh -T $RECOVERY_TARGET \
    "[ -d $RECOVERY_DATA ] && mv $RECOVERY_DATA $RECOVERY_DATA.$TS"

# We only use archived WAL logs during recoveries, so delete all
# logs from the last recovery to limit the growth.
rm $MASTER_DATA/archive_log/*

# With this file present, our archive_command will actually
# archive WAL files.
touch $MASTER_DATA/archive_log/backup_in_progress

# Perform a backup of the database.
ssh -T $RECOVERY_TARGET \
    "pg_basebackup -h $MASTER_HOST -D $RECOVERY_DATA --xlog"

# Configure the restore_command to use the archive_log WALs we’ll copy
# over in 2nd_stage.sh.
echo "restore_command = 'cp $RECOVERY_DATA/archive_log/%f %p'" | \
    ssh -T $RECOVERY_TARGET "cat > $RECOVERY_DATA/recovery.conf"
EOF
</pre>

<pre>
cat >/var/lib/postgresql/9.4/main/2nd_stage.sh <<'EOF'
#! /bin/sh
MASTER_DATA=$1
RECOVERY_TARGET=$2
RECOVERY_DATA=$3
port=5432

# Force to flush current value of sequences to xlog
psql -p $port -t -c 'SELECT datname FROM pg_database WHERE NOT datistemplate AND datallowconn' template1|
while read i
do
  if [ "$i" != "" ];then
    psql -p $port -c "SELECT setval(oid, nextval(oid)) FROM pg_class WHERE relkind = 'S'" $i
  fi
done

# Flush all transactions to disk. Since pgpool stopped all connections,
# there cannot be any data that does not reside on disk until the
# to-be-recovered host is back on line.
psql -p $port -c "SELECT pgpool_switch_xlog('$MASTER_DATA/archive_log')" template1

# Copy over all archive logs at once.
rsync -avx --delete $MASTER_DATA/archive_log/ \
    $RECOVERY_TARGET:$RECOVERY_DATA/archive_log/

# Delete the flag file to disable WAL archiving again.
rm $MASTER_DATA/archive_log/backup_in_progress
EOF
</pre>

<pre>
cat >/var/lib/postgresql/9.4/main/pgpool_remote_start <<'EOF'
#!/bin/sh
ssh $1 sudo systemctl start postgresql.service
EOF

chmod +x /var/lib/postgresql/9.4/main/1st_stage.sh
chmod +x /var/lib/postgresql/9.4/main/2nd_stage.sh
chmod +x /var/lib/postgresql/9.4/main/pgpool_remote_start
</pre>

<p>
Now, let’s start pgpool2 and verify that it works and that we can access our
first node. The <code>pcp_node_count</code> command should return an integer
number like “2”. The psql command should be able to connect and you should see
your database tables when using <code>\d</code>.
</p>
<pre>
systemctl restart pgpool2.service
pcp_node_count 10 localhost 9898 postgres wQgvBEusf1NWDRKVXS15Fc8
psql -p 5433 -U faq_i3wm_org faq_i3wm_org
</pre>

<h3>Monitoring</h3>

<p>
pgpool2 intercepts a couple of SHOW statements, so you can use the SQL command
<code>SHOW pool_nodes</code> to see how many nodes are there:
</p>

<pre>
&gt; SHOW pool_nodes;
 node_id |     hostname     | port | status | lb_weight |  role  
---------+------------------+------+--------+-----------+--------
 0       | midna.zekjur.net | 5432 | 2      | 0.666667  | master
 1       | alp.zekjur.net   | 5432 | 2      | 0.333333  | slave
(2 rows)
</pre>

<p>
You could export a cgi-script over HTTP, which just always runs this command,
and then configure your monitoring software to watch for certain strings in the
output. Note that you’ll also need to configure a <code>~/.pgpass</code> file
for the <code>www-data</code> user. As an example, to monitor whether
<code>alp</code> is still a healthy backend, match for “alp.zekjur.net,5432,2”
in the output of this script:
</p>

<pre>
#!/bin/sh
cat &lt;&lt;'EOT'
Content-type: text/plain

EOT
exec echo 'SHOW pool_nodes;' | psql -t -A -F, --host localhost \
  -U faq_i3wm_org faq_i3wm_org
</pre>

<h3>Performing/Debugging a recovery</h3>

<p>
In order to recover node 1 (<code>alp</code> in this case), use:

<pre>
pcp_recovery_node 300 localhost 9898 postgres wQgvBEusf1NWDRKVXS15Fc8 1
</pre>

<p>
The “300” used to be a timeout, but these days it’s only supported for
backwards compatibility and has no effect.
</p>

<p>
In case the recovery fails, the only thing you’ll get back from
pcp_recovery_node is the text “BackendError”, which is not very helpful. The
logfile of pgpool2 contains a bit more information, but to debug recovery
problems, I typically strace all PostgreSQL processes and see what the scripts
are doing/where they are failing.
</p>

<h3>pgpool2 behavior during recovery</h3>

<p>
In order to see how pgpool2 performs during recovery/degradation, you can use
<a href="http://play.golang.org/p/xwMxt-BiDU">this little Go program</a> that
tries to do three things every 0.25 seconds: check that the database is healthy
(<code>SELECT 1;</code>), run a meaningful SELECT, run an UPDATE.
</p>

<p>
When a database node goes down, a single query may fail until pgpool2 realizes
that the node needs to be degraded. If your database load is light, chances are
that pgpool2 will realize the database is down without even failing a single
query, though.
</p>

<pre>
2014-08-13 23:15:27.638 health: ✓  select: ✓  update: ✓
2014-08-13 23:15:28.700 insert failed: driver: bad connection
2014-08-13 23:15:28.707 health: ✓  select: ✓  update: x
</pre>

<p>
During recovery, there is a time when pgpool2 will just disconnect all clients
and not answer any queries any more (2nd stage). In this case, the state lasted
for about 20 seconds:
</p>

<pre>
…
2014-08-13 23:16:01.900 health: ✓  select: ✓  update: ✓
2014-08-13 23:16:02.161 health: ✓  select: ✓  update: ✓
# no queries answered here
2014-08-13 23:16:23.625 health: ✓  select: ✓  update: ✓
2014-08-13 23:16:24.308 health: ✓  select: ✓  update: ✓
…
</pre>

<h3>Conclusion</h3>

<p>
Setting up a PostgreSQL setup that involves pgpool2 is definitely a lot of
work. It could be a bit easier if the documentation was more specific on the
details of how recovery is supposed to work and would include the configuration
that I came up with above. Ideally, something like pgpool2 would be part of
PostgreSQL itself.
</p>

<p>
I am not yet sure how much software I’ll need to touch in order to make it
gracefully deal with the PostgreSQL connection dying and coming back up. I know
of at least one program I use (buildbot) which does not handle this situation
well at all — it needs a complete restart to work again.
</p>

<p>
Time will tell if the setup is stable and easy to maintain. In case I make
negative experiences, I’ll update this article :).
</p>

</div></div></div></div></div></div></body>
</html>
